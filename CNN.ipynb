{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7enaWG05_X2Z",
        "colab_type": "text"
      },
      "source": [
        "# My first Google Colab Notebook. Also my first implementation of a CNN with Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYPxuj6tJdG6",
        "colab_type": "code",
        "outputId": "34440e21-b4c5-4dc0-a2f9-cad08b64a66b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "'''The data is loaded into Google Drive. Data can be directly downloaded from\n",
        "Kaggle using Kaggle API. However in this proejct I have downloaded the training\n",
        "and testing data for the MNIST dataset (considered the \"Hello World\" of Machine\n",
        "Learning) in my Google Drive. The Drive is loaded in the notebook\n",
        "'''\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "drive.mount('/content/gdrive', force_remount = True)\n",
        "dataset_path = 'gdrive/My Drive/Projects/MNIST/'\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlVXDy0YVzzT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#The test and training files are loaded as  dataframes\n",
        "train_file = dataset_path + \"train.csv\"\n",
        "test_file = dataset_path + \"test.csv\"\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ql7eSYVgtfKY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv(train_file)\n",
        "test = pd.read_csv(test_file)\n",
        "\n",
        "#Training data is prepared for the labels as well as the pixel wise values\n",
        "Y_train = train['label']\n",
        "X_train = train.drop(labels = [\"label\"],axis = 1) \n",
        "\n",
        "#The pixel values are normalized\n",
        "X_train = X_train/255.0\n",
        "test = test/255.0\n",
        "\n",
        "#As we will be feeding in images we reshape the vector to 28x28 images\n",
        "X_train = X_train.values.reshape(-1, 28, 28, 1)\n",
        "test = test.values.reshape(-1, 28, 28, 1)\n",
        "\n",
        "#Using the following the labels are converted to one-hot encoded form\n",
        "Y_train = pd.get_dummies(Y_train)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDx2xUlsBehg",
        "colab_type": "text"
      },
      "source": [
        "# All the different constituents for building the model are loaded\n",
        "\n",
        "\n",
        "*   **train_test_split** is loaded to get validation data and training data\n",
        "*   **Sequential** is used as we are building a Sequential model where one layer connects to the next layer\n",
        "* **Dense, Dropout, Flatten, Conv2D, MaxPool2D** are all used to build the CNN as we require **fully-connected layers**, we are adding **dropout** to prevent overfitting. To convert from convolution layers to fully-connected layers we require **Flatten**. Finally **Conv2D, MaxPool2D** are required as we want a CNN with Max Pooling features\n",
        "* Two Optimizers **RMSProp** and **Adam** are loaded. **Adam** has been used finally\n",
        "* **ImageDataGenerator** is a Keras trick to provide dynamic **Data Augmentation**\n",
        "* **ReduceLROnPlateau** is used to reduce the Learning Rate when **Validation Loss** does not reduce after certain number of epochs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-g4Taio6vL3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
        "from keras.optimizers import RMSprop, Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ReduceLROnPlateau"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFpOuYvSwKcm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(2)\n",
        "random_seed = 2\n",
        "#Training and Validation data is generated\n",
        "\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train,\n",
        "                                                  test_size = 0.1,\n",
        "                                                  random_state=random_seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0t1EvV5WE2yG",
        "colab_type": "text"
      },
      "source": [
        "# CNN is created here\n",
        "* **Conv Layer-filter size:(5x5), num of filters: 32, activation: relu**\n",
        "* **Conv Layer-filter size:(5x5), num of filters: 32, activation: relu**\n",
        "* **Max Pool Layer with filter size (2x2)**\n",
        "* **Dropout with probability of 0.25 for ignoring neurons**\n",
        "\n",
        "* **Conv Layer-filter size:(3x3), num of filters: 64, activation: relu**\n",
        "* **Conv Layer-filter size:(3x3), num of filters: 64, activation: relu**\n",
        "* **Max Pool Layer with filter size (2x2) with strides of (2x2)**\n",
        "* **Dropout with probability of 0.25 for ignoring neurons**\n",
        "\n",
        "* After flattening we connect to a **Dense Layer of 256 neurons** and a **dropout of 50%**\n",
        "* Finally we connect to a layer of **10 neurons** as we have **10 classes**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtpzdtcxx1Fl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
        "                 activation ='relu', input_shape = (28,28,1)))\n",
        "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
        "                 activation ='relu'))\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "\n",
        "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
        "                 activation ='relu'))\n",
        "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
        "                 activation ='relu'))\n",
        "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation = \"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10, activation = \"softmax\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USBzrwzQyfDQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Adam Optimizer is set\n",
        "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0,\n",
        "                 amsgrad=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epb3RoSUym42",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''The loss that we are trying to minimize is the categorical cross entropy\n",
        "loss. The metric that we are looking at is accuracy\n",
        "'''\n",
        "model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIAgH4ZnysxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''We are monitoring the validation accuracy. If it does not improve for\n",
        "3 epochs we reduce the Learning rate by half. The minimum learning rate is\n",
        "1e-5\n",
        "'''\n",
        "\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
        "                                            patience=3, \n",
        "                                            verbose=1, \n",
        "                                            factor=0.5, \n",
        "                                            min_lr=0.00001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYe762-9ytzk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 30\n",
        "batch_size = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UNRBuRNyyeF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Data augmentation happening here. The data generator is fitted with the \n",
        "training data and the images are modified randomly to generate the data for\n",
        "a particular epoch\n",
        "'''\n",
        "\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        zoom_range = 0.1, # Randomly zoom image \n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=False,  # randomly flip images\n",
        "        vertical_flip=False)  # randomly flip images\n",
        "\n",
        "\n",
        "datagen.fit(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8ZK_9TRy78y",
        "colab_type": "code",
        "outputId": "3f11cdbc-1c4f-430c-88b5-2a4348445f3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1231
        }
      },
      "source": [
        "'''The step that fits and generates the model to X_train and Y_train.\n",
        "callbacks mentions anything that we want to run in the midst of the training\n",
        "process depending on the status of the training values\n",
        "'''\n",
        "\n",
        "history = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size),\n",
        "                              epochs = epochs, validation_data = (X_val,Y_val),\n",
        "                              verbose = 2, steps_per_epoch=X_train.shape[0] // batch_size\n",
        "                              , callbacks=[learning_rate_reduction])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            " - 7s - loss: 0.5149 - acc: 0.8334 - val_loss: 0.0822 - val_acc: 0.9743\n",
            "Epoch 2/30\n",
            " - 7s - loss: 0.1543 - acc: 0.9538 - val_loss: 0.0511 - val_acc: 0.9836\n",
            "Epoch 3/30\n",
            " - 7s - loss: 0.1073 - acc: 0.9685 - val_loss: 0.0476 - val_acc: 0.9865\n",
            "Epoch 4/30\n",
            " - 7s - loss: 0.0897 - acc: 0.9734 - val_loss: 0.0385 - val_acc: 0.9899\n",
            "Epoch 5/30\n",
            " - 7s - loss: 0.0782 - acc: 0.9763 - val_loss: 0.0399 - val_acc: 0.9889\n",
            "Epoch 6/30\n",
            " - 7s - loss: 0.0703 - acc: 0.9788 - val_loss: 0.0399 - val_acc: 0.9905\n",
            "Epoch 7/30\n",
            " - 7s - loss: 0.0596 - acc: 0.9825 - val_loss: 0.0386 - val_acc: 0.9902\n",
            "Epoch 8/30\n",
            " - 7s - loss: 0.0585 - acc: 0.9825 - val_loss: 0.0350 - val_acc: 0.9902\n",
            "Epoch 9/30\n",
            " - 7s - loss: 0.0545 - acc: 0.9844 - val_loss: 0.0335 - val_acc: 0.9921\n",
            "Epoch 10/30\n",
            " - 8s - loss: 0.0554 - acc: 0.9828 - val_loss: 0.0339 - val_acc: 0.9913\n",
            "Epoch 11/30\n",
            " - 7s - loss: 0.0488 - acc: 0.9852 - val_loss: 0.0316 - val_acc: 0.9910\n",
            "Epoch 12/30\n",
            " - 7s - loss: 0.0499 - acc: 0.9851 - val_loss: 0.0329 - val_acc: 0.9913\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "Epoch 13/30\n",
            " - 7s - loss: 0.0352 - acc: 0.9895 - val_loss: 0.0269 - val_acc: 0.9934\n",
            "Epoch 14/30\n",
            " - 7s - loss: 0.0328 - acc: 0.9904 - val_loss: 0.0294 - val_acc: 0.9931\n",
            "Epoch 15/30\n",
            " - 7s - loss: 0.0334 - acc: 0.9898 - val_loss: 0.0272 - val_acc: 0.9937\n",
            "Epoch 16/30\n",
            " - 7s - loss: 0.0338 - acc: 0.9893 - val_loss: 0.0249 - val_acc: 0.9934\n",
            "Epoch 17/30\n",
            " - 7s - loss: 0.0306 - acc: 0.9909 - val_loss: 0.0287 - val_acc: 0.9926\n",
            "Epoch 18/30\n",
            " - 7s - loss: 0.0295 - acc: 0.9911 - val_loss: 0.0300 - val_acc: 0.9931\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "Epoch 19/30\n",
            " - 7s - loss: 0.0256 - acc: 0.9917 - val_loss: 0.0270 - val_acc: 0.9934\n",
            "Epoch 20/30\n",
            " - 7s - loss: 0.0237 - acc: 0.9929 - val_loss: 0.0274 - val_acc: 0.9937\n",
            "Epoch 21/30\n",
            " - 8s - loss: 0.0222 - acc: 0.9931 - val_loss: 0.0274 - val_acc: 0.9929\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "Epoch 22/30\n",
            " - 7s - loss: 0.0225 - acc: 0.9933 - val_loss: 0.0253 - val_acc: 0.9939\n",
            "Epoch 23/30\n",
            " - 7s - loss: 0.0223 - acc: 0.9932 - val_loss: 0.0270 - val_acc: 0.9939\n",
            "Epoch 24/30\n",
            " - 7s - loss: 0.0199 - acc: 0.9935 - val_loss: 0.0267 - val_acc: 0.9942\n",
            "Epoch 25/30\n",
            " - 7s - loss: 0.0214 - acc: 0.9934 - val_loss: 0.0288 - val_acc: 0.9931\n",
            "Epoch 26/30\n",
            " - 7s - loss: 0.0213 - acc: 0.9937 - val_loss: 0.0290 - val_acc: 0.9934\n",
            "Epoch 27/30\n",
            " - 7s - loss: 0.0209 - acc: 0.9938 - val_loss: 0.0267 - val_acc: 0.9939\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "Epoch 28/30\n",
            " - 7s - loss: 0.0172 - acc: 0.9945 - val_loss: 0.0276 - val_acc: 0.9942\n",
            "Epoch 29/30\n",
            " - 7s - loss: 0.0176 - acc: 0.9945 - val_loss: 0.0281 - val_acc: 0.9942\n",
            "Epoch 30/30\n",
            " - 7s - loss: 0.0189 - acc: 0.9943 - val_loss: 0.0270 - val_acc: 0.9934\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFnrCZYFzA2S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = model.predict(test)#predict for the test data\n",
        "\n",
        "# take the index of highest probability \n",
        "results = np.argmax(results,axis = 1)\n",
        "#Convert the array to a pandas series\n",
        "results = pd.Series(results,name=\"Label\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLk_Cz-c0Ozj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Saving the output file.\n",
        "\n",
        "submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n",
        "\n",
        "submission.to_csv(\"cnn_mnist_datagen.csv\",index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kD2ktyeM0TkM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
